{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fully Connected Neural Network (FCNN) Architectures\n",
        "\n",
        "## In this Notebook is the framework for four different FCNN models. Here they can be editted and changed for use in main program."
      ],
      "metadata": {
        "id": "DKdghckk9-Jo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kas3r_8_9y8k"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Define input_dim, the number of possible genes\n",
        "\n",
        "# Define the FCNN Class Called TLS_Identifier_CNN\n",
        "# This First FCNN is Simple, 4 total layers, 2 hidden layers, with ReLU, Drop out and Sigmoid Binary Classification.\n",
        "# Number of Hidden Layers:\n",
        "#     Too few layers: The model may be unable to capture complex patterns.\n",
        "#     Too many layers: The model may overfit or become computationally expensive.\n",
        "\n",
        "# Test combinations of layers and neurons. For example:\n",
        "\n",
        "# Layers: [1, 2, 3]\n",
        "# Neurons: [64, 128, 256]\n",
        "# Example combinations:\n",
        "\n",
        "# 1 layer with 64 neurons.\n",
        "# 2 layers with 128 → 64 neurons.\n",
        "# 3 layers with 256 → 128 → 64 neurons.\n",
        "class Simple_FCNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "      super(Simple_FCNN, self).__init__()\n",
        "\n",
        "      # Define Input Layer which will pass information to hidden layers. Going from n-number of Genes to 128 neurons\n",
        "      self.fc1 = nn.Linear(input_dim, 128)  # Fully connected layer\n",
        "      self.relu = nn.ReLU()                 # ReLU (Rectified Linear Unit) activation for non-linearity\n",
        "\n",
        "      # Hidden Layer 1 to Hidden Layer 2 (64 neurons)\n",
        "      # Creating drop out layer to hopefully avoid overfitting by randomly dropping neurons throughout training\n",
        "      self.fc2 = nn.Linear(128, 64)        # Fully connected layer\n",
        "      self.dropout = nn.Dropout(0.3)       # Dropout for regularization\n",
        "\n",
        "      # Hidden Layer 2 to Output Layer (1 neuron)\n",
        "      # Applying signmoid layer to output for final binary classification, TLS(1), or NO_TLS(0)\n",
        "      self.fc3 = nn.Linear(64, 1)           # Fully connected layer (1 neuron for binary classification)\n",
        "      self.sigmoid = nn.Sigmoid()           # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Define the forward pass\n",
        "      x = self.relu(self.fc1(x))            # Apply ReLU after first layer\n",
        "      x = self.dropout(x)                   # Apply Dropout\n",
        "      x = self.relu(self.fc2(x))            # Apply ReLU after second layer\n",
        "      x = self.dropout(x)                   # Apply Dropout again\n",
        "      x = self.sigmoid(self.fc3(x))         # Apply Sigmoid for final probability output\n",
        "      return x\n",
        "\n",
        "# Creating another FCNN called Deep_FCNN which has 4 layers and increased amounts of neurons\n",
        "# This model will be more computationally expensive\n",
        "class Deep_FCNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "      super(Deep_FCNN, self).__init__()\n",
        "      self.fc1 = torch.nn.Linear(input_dim, 256)  # Increase neurons\n",
        "      self.fc2 = torch.nn.Linear(256, 128)\n",
        "      self.fc3 = torch.nn.Linear(128, 64)\n",
        "      self.fc4 = torch.nn.Linear(64, 1)\n",
        "      self.relu = torch.nn.ReLU()\n",
        "      self.sigmoid = torch.nn.Sigmoid()\n",
        "      self.dropout = torch.nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.relu(self.fc1(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.relu(self.fc2(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.relu(self.fc3(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.sigmoid(self.fc4(x))\n",
        "      return x\n",
        "\n",
        "\n",
        "# Creating another FCNN called BatchNorm_FCNN which is similar to deep FCNN, but adds a batch normalization layer after each hidden layer\n",
        "# Normalizes activations during training, which can accelerate convergence\n",
        "# Reduces internal covariate shift, improving generalization\n",
        "class BatchNorm_FCNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "      super(BatchNorm_FCNN, self).__init__()\n",
        "      self.fc1 = torch.nn.Linear(input_dim, 256)\n",
        "      self.bn1 = torch.nn.BatchNorm1d(256)  # Batch normalization, after head hidden layer\n",
        "      self.fc2 = torch.nn.Linear(256, 128)\n",
        "      self.bn2 = torch.nn.BatchNorm1d(128)\n",
        "      self.fc3 = torch.nn.Linear(128, 64)\n",
        "      self.bn3 = torch.nn.BatchNorm1d(64)\n",
        "      self.fc4 = torch.nn.Linear(64, 1)\n",
        "      self.relu = torch.nn.ReLU()\n",
        "      self.sigmoid = torch.nn.Sigmoid()\n",
        "      self.dropout = torch.nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.relu(self.bn1(self.fc1(x)))  # Batch normalization after fc1, and again after each layer\n",
        "      x = self.dropout(x)\n",
        "      x = self.relu(self.bn2(self.fc2(x)))\n",
        "      x = self.dropout(x)\n",
        "      x = self.relu(self.bn3(self.fc3(x)))\n",
        "      x = self.dropout(x)\n",
        "      x = self.sigmoid(self.fc4(x))\n",
        "      return x\n",
        "\n",
        "# Creating another FCNN called Residual_FCNN\n",
        "# Adding residual connections to the model, allowing the network to learn idenity mappig\n",
        "# This may help mitigate vanishing gradient in the deeper network\n",
        "# Allowing the model to retain learned patterns from earlier layers\n",
        "class Residual_FCNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "      super(Residual_FCNN, self).__init__()\n",
        "      self.fc1 = torch.nn.Linear(input_dim, 256)\n",
        "      self.fc2 = torch.nn.Linear(256, 128)\n",
        "      self.fc3 = torch.nn.Linear(128, 64)\n",
        "      self.fc4 = torch.nn.Linear(64, 1)\n",
        "      self.relu = torch.nn.ReLU()\n",
        "      self.sigmoid = torch.nn.Sigmoid()\n",
        "      self.dropout = torch.nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Applying a linear transformation to the input to match the dimensions of fc2 output\n",
        "      # This creates a new \"identity\" that has the correct dimensions\n",
        "      identity = self.fc1(x)\n",
        "      identity = self.relu(identity) # Applying activation to match post-activation output\n",
        "      identity = self.dropout(identity) # Applying regularization, and matching fc2 input dimension\n",
        "      identity = self.fc2(identity)\n",
        "      # Performing the rest of the forward pass as before\n",
        "      x = self.relu(self.fc1(x))\n",
        "      x = self.dropout(x)\n",
        "      # Adding the modified \"identity\" here which now has the correct dimensions\n",
        "      x = self.relu(self.fc2(x) + identity)  # Residual connection\n",
        "      x = self.dropout(x)\n",
        "      x = self.relu(self.fc3(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.sigmoid(self.fc4(x))\n",
        "      return x"
      ]
    }
  ]
}